{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm 2 Assignment 3 Davide Amadei"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Iterator\n",
    "import time\n",
    "from sewar.full_ref import uqi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Mean Squared Error\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        first input\n",
    "    y : np.ndarray\n",
    "        second input\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        returns the Mean Squared Error computed on x and y\n",
    "    \"\"\"\n",
    "    return ((x-y)**2).mean()\n",
    "\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"sigmoid function applied elementwise to a matrix or vector\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        input matrix or vector\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        matrix or vector of same size of input containing elementwise sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def plot_images(indices: list[int], og:np.ndarray, recon:np.ndarray, \n",
    "                labels:np.ndarray, figsize:tuple[float, float]=None) -> None:\n",
    "    \"\"\"simple function to plot comparisons between original images and the ones reconstructed by the RBM\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    indices : list[int]\n",
    "        list of indices of the data to be shown\n",
    "    og : np.ndarray\n",
    "        original images\n",
    "    recon : np.ndarray\n",
    "        reconstructed images\n",
    "    labels : np.ndarray\n",
    "        labels of the data\n",
    "    figsize : tuple[float, float], optional\n",
    "        size of the image to be shown. If None use (len(indices)*1.5, 3) as size, by default None\n",
    "    \"\"\"\n",
    "    if figsize is None:\n",
    "        figsize = (len(indices)*1.5, 3)\n",
    "\n",
    "    fig, axes = plt.subplots(2, len(indices), figsize=figsize)\n",
    "\n",
    "    i = 0\n",
    "    for index in indices:\n",
    "        loss = np.round(mse(og[index], recon[index]), 4)\n",
    "        uqi_loss = np.round(uqi(og[index], recon[index]), 4)\n",
    "\n",
    "        # row 1, original image\n",
    "        ax = axes[0, i]\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"{labels[index]}\")\n",
    "        ax.imshow(og[index], cmap=\"gray\")\n",
    "\n",
    "        # row 2, reconstructed image\n",
    "        ax = axes[1, i]\n",
    "            \n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        ax.set_xlabel(f\"MSE: {loss}\\n\"\n",
    "                      f\"UQI: {uqi_loss}\")\n",
    "        \n",
    "        ax.imshow(recon[index], cmap=\"gray\")\n",
    "        i+=1\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to read the MNIST dataset\n",
    "As the MNIST dataset is provided in idx format it has to be read from the original file to be converted into a usable form.<br>\n",
    "This function does exactly that, reading and decoding the file based on the format explained on the [MNIST site](http://yann.lecun.com/exdb/mnist/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_MNIST_file(path: str, reshape: bool = False) -> tuple[tuple, np.ndarray]:\n",
    "    \"\"\"simple utility function to read a file containing MNIST datasets and labels\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        path containing the file to load\n",
    "    reshape : bool, optional\n",
    "        flag determining wheter the dataset is returned already reshaped as images or flattened.\n",
    "        If True each element of the dataset is reshaped to a matrix of size 28x28, \n",
    "        else each element is a vector of size 784, by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[tuple, np.ndarray]\n",
    "        the shape of the output data and either the dataset or the labels read from the file\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        if the magic numbers of the file are wrong\n",
    "    \"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        magic = int.from_bytes(f.read(4))\n",
    "        n_items = int.from_bytes(f.read(4))\n",
    "\n",
    "        # image file\n",
    "        if magic == 2051:\n",
    "            n_rows = int.from_bytes(f.read(4))\n",
    "            n_columns = int.from_bytes(f.read(4))\n",
    "\n",
    "            # if reshape is true change the output shape\n",
    "            if reshape:\n",
    "                shape = (n_items, n_rows, n_columns)\n",
    "            else:\n",
    "                shape = (n_items, n_rows * n_columns)\n",
    "            images = np.ndarray(shape)\n",
    "\n",
    "            # read each image one by one\n",
    "            for i in range(n_items):\n",
    "                img_buffer = f.read(n_rows*n_columns)\n",
    "                # convert binary data buffer to numpy int array\n",
    "                img = np.frombuffer(img_buffer, np.uint8)\n",
    "\n",
    "                # reshape the image if needed\n",
    "                if reshape:\n",
    "                    img = np.reshape(img, (n_rows, n_columns))\n",
    "\n",
    "                images[i] = img\n",
    "            # the images are returned already normalized between 0 and 1,\n",
    "            # necessary for data to be usable\n",
    "            return (shape, images/255)\n",
    "        # label file\n",
    "        elif magic == 2049:\n",
    "            label_buffer = f.read(n_items)\n",
    "            # convert binary data buffer to numpy int array\n",
    "            labels = np.frombuffer(label_buffer, np.uint8)\n",
    "            return (labels.shape, labels)\n",
    "        # wrong type of file\n",
    "        else:\n",
    "            raise ValueError(\"wrong file format\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training set and the test set with its labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_img_shape, tr_images = read_MNIST_file(\"./data/train-images-idx3-ubyte\")\n",
    "ts_img_shape, ts_images = read_MNIST_file(\"./data/t10k-images-idx3-ubyte\")\n",
    "_, test_labels = read_MNIST_file(\"./data/t10k-labels-idx1-ubyte\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricted Boltzmann Machine implementation\n",
    "This is the implementation of the Restricted Boltzmann Machine and all the needed methods to train it and to use it for reconstructing data outside of the training set.<br>\n",
    "All the sampling is done binarizing the data as seen in the code example in class. This means starting from an array of elements $0 \\leq x \\leq 1$  $\\forall x$ and using them as probablities to get a binary array where each $x$ is used as the probabliity of being sampled to 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative measures for the error\n",
    "\n",
    "To get a quantitive measure of the error of the RBM during training and on the test data two measures are used. The first one is simply the MSE, compute for each batch and then averaged at the end of the epoch. The other measure is the Universal image Quality Index (UQI). It always returns a value between -1 and 1, with 1 being that the two images are the same. The implementation found in the [sewar library](https://github.com/andrewekhalel/sewar) was used. More on this index can be found on the [paper it was introduced in](https://ieeexplore.ieee.org/document/995823). I decided to use this in addition to MSE to get a possibly easier to interpret score of the similarity between the original image and the one reconstructed by the RBM.<br>\n",
    "Unfortunately this measure needs to be computed on each image separately, which also need to be reshaped for the UQI to work. This takes a lot of time overall and slows down the training tremendously, as it takes around 30 seconds in each epoch to compute this measure."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of the weights\n",
    "The weights and the biases for the hidden units were initialized as outlined [here](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf). This means that the weights are initialized with values sampled from a zero-centered Gaussian. Its standard deviation is 0.001 instead of 0.01 as suggested in the guide because it gives slightly better results when combined with the other hyperparameters used during training. The biases of the hidden units are initialized with all zeros. The biases of the visible units are initialized the same way as the weights for the sake of simplicity, as doing it as suggested in the guide would complicate things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM:\n",
    "    \"\"\"\n",
    "    class implementing a Restricted Boltzmann Machine with a method to fit it on given data \n",
    "    and a method to reconstruct data given in input\n",
    "    \"\"\"\n",
    "    def __init__(self, v_units: int, h_units:int = 100, seed:int = None):\n",
    "        \"\"\"init method\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        v_units : int\n",
    "            number of visible units, must be the same as the size of a single element of training data\n",
    "        h_units : int\n",
    "            number of hidden units, by default 100\n",
    "        seed : int, optional\n",
    "            seed to initialize RNG, by default None\n",
    "        \"\"\"\n",
    "        self._rng = np.random.default_rng(seed)\n",
    "        self._seed = seed\n",
    "\n",
    "        self._weights = self._rng.normal(size=(v_units, h_units), scale=1e-3)\n",
    "        self._old_delta_weights = np.zeros(shape=(v_units, h_units))\n",
    "\n",
    "        self._bv = self._rng.normal(size=v_units, scale=1e-3)\n",
    "        self._old_delta_bv = np.zeros(shape=v_units)\n",
    "\n",
    "        self._bh = np.zeros(shape=h_units)\n",
    "        self._old_delta_bh = np.zeros(shape=h_units)\n",
    "        # self._bv = self._rng.uniform(0, 1, size=v_units)\n",
    "        # self._bh = self._rng.uniform(0, 1, size=h_units)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_minibatches(\n",
    "        x: np.ndarray, batchsize: int\n",
    "    ) -> Iterator[tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"Returns minibatches of given size over (x, y).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            data array\n",
    "        batchsize : int\n",
    "            batch size of yielded minibatches\n",
    "\n",
    "        Yields\n",
    "        ------\n",
    "        Iterator[tuple[np.ndarray,np.ndarray]]\n",
    "            iterator over minibatches\n",
    "        \"\"\"\n",
    "        if batchsize in [None, 0, -1]:\n",
    "            batchsize = x.shape[0]\n",
    "        size = x.shape[0]\n",
    "        batchtotal, remainder = divmod(size, batchsize)\n",
    "        for i in range(batchtotal):\n",
    "            mini_x = x[i * batchsize : (i + 1) * batchsize]\n",
    "            yield mini_x\n",
    "        if remainder > 0:\n",
    "            yield (x[batchtotal * batchsize :])\n",
    "\n",
    "    def sample_data(self, data: np.ndarray, seed: int = None) -> np.ndarray:\n",
    "        \"\"\"method to get a sample of binary data from a given array of elements x where 0 <= x <= 1.\n",
    "        These elements are seen as probabilities and become 0 or 1 indipendently of each other.\n",
    "        P(x->1) = x, P(x->0) = 1-x \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : np.ndarray\n",
    "            data to sample\n",
    "        seed : int, optional\n",
    "            seed to use for sampling, if None uses the RNG of the current obect, by default None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            an array containing the sampled data. The elements of this array are either 0 or 1\n",
    "        \"\"\"\n",
    "        if seed is None:\n",
    "            return self._rng.binomial(1, p=data)\n",
    "        else:\n",
    "            tmp_rng = np.random.default_rng(seed)\n",
    "            return tmp_rng.binomial(1, p=data)\n",
    "        \n",
    "    def reset(self, seed: int = None):\n",
    "        \"\"\"method to reset the RBM object by reinitializing weights and RNG\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seed : int, optional\n",
    "            seed to use for RNG reinitialization. If none uses old seed, by default None\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            self._seed = seed\n",
    "        self._rng = np.random.default_rng(self._seed)\n",
    "\n",
    "        self._weights = self._rng.normal(size=self._weights.shape, scale=1e-3)\n",
    "        self._old_delta_weights = np.zeros(shape=self._weights.shape)\n",
    "\n",
    "        self._bv = self._rng.normal(size=self._bv.shape, scale=1e-3)\n",
    "        self._old_delta_bv = np.zeros(shape=self._bv.shape)\n",
    "\n",
    "        self._bh = np.zeros(shape=self._bh.shape)\n",
    "        self._old_delta_bh = np.zeros(shape=self._bh.shape)\n",
    "        \n",
    "    def fit(self, tr_data: np.ndarray, max_epochs=10, lr: float = 1, momentum:float = 0.9, batchsize=128, img_shape: tuple = None):\n",
    "        \"\"\"function to train the RBM\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tr_data : np.ndarray\n",
    "            data to use for training\n",
    "        max_epochs : int, optional\n",
    "            max number of epochs, by default 10\n",
    "        lr : float, optional\n",
    "            learning rate, multiplies the updates to the weights, by default 1\n",
    "        momentum : float, optional\n",
    "            momentum coefficient, by default 0.9\n",
    "        batchsize : int, optional\n",
    "            size of each batch, by default 128\n",
    "        img_shape : tuple, optional\n",
    "            shape of the images that are being processed during training.\n",
    "            If not None in addition to the MSE the Universal image Quality Index (UQI) is also computed, by default None\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "\n",
    "        # keep track of the total time spent on UQI computation\n",
    "        tot_uqi_time = 0\n",
    "\n",
    "        for i in range(max_epochs):\n",
    "            # shuffle data for the current epoch\n",
    "            shuffled_data = tr_data[self._rng.permutation(tr_data.shape[0])]\n",
    "\n",
    "            loss_list = []\n",
    "\n",
    "            # if image shape is passed UQI score is also computed\n",
    "            if img_shape is not None:\n",
    "                uqi_loss_list = []\n",
    "                epoch_uqi_time = 0\n",
    "\n",
    "            # iterate on batches\n",
    "            for batch in RBM.get_minibatches(shuffled_data, batchsize):\n",
    "                \n",
    "                # sample the training data of the current batch\n",
    "                sampled_data = self.sample_data(batch)\n",
    "                # p(h0 | v0)\n",
    "                h_prob0 = sigmoid(np.dot(sampled_data, self._weights) + self._bh)\n",
    "                # sample the output of the hidden units from p(h0 | v0)\n",
    "                sampled_h0 = self.sample_data(h_prob0)\n",
    "\n",
    "                # wake part of the weight update\n",
    "                wake = np.dot(sampled_data.T, h_prob0)\n",
    "\n",
    "                # p(v1 | h0)\n",
    "                recon_data_prob = sigmoid(np.dot(sampled_h0, self._weights.T) + self._bv)\n",
    "                # sample the reconstructed data from p(v1 | h0)\n",
    "                recon_data = self.sample_data(recon_data_prob)\n",
    "\n",
    "                # p(h1 | v1)\n",
    "                h_prob1 = sigmoid(np.dot(recon_data, self._weights) + self._bh)\n",
    "                # sampled_h1 = self.sample_data(h_prob1)\n",
    "                \n",
    "                #dream part of the weight update\n",
    "                dream = np.dot(recon_data.T, h_prob1)\n",
    "\n",
    "                delta_weights = (lr * (wake - dream)/batch.shape[0])\n",
    "                delta_bv = (lr * (np.sum(sampled_data, axis=0) - \n",
    "                                  np.sum(recon_data, axis=0))/batch.shape[0])\n",
    "                delta_bh = (lr * (np.sum(h_prob0, axis=0) - \n",
    "                                  np.sum(h_prob1, axis=0))/batch.shape[0])\n",
    "\n",
    "                # update weights\n",
    "                self._weights += delta_weights + momentum*self._old_delta_weights\n",
    "                self._bv += delta_bv + momentum*self._old_delta_bv\n",
    "                self._bh += delta_bh + momentum*self._old_delta_bh\n",
    "\n",
    "                # keep old updates for momentum\n",
    "                self._old_delta_weights = delta_weights\n",
    "                self._old_delta_bv = delta_bv\n",
    "                self._old_delta_bh = delta_bh\n",
    "                \n",
    "                # compute UQI score and keep track of time spent on its computation\n",
    "                if img_shape != None:\n",
    "                    uqi_time_start = time.time()\n",
    "                    uqi_loss_batch = 0\n",
    "                    # reshape the data to compute UQI\n",
    "                    reshaped_batch = np.reshape(batch, (batch.shape[0],) + img_shape)\n",
    "                    reshaped_recon_data = np.reshape(recon_data_prob, (recon_data_prob.shape[0],) + img_shape)\n",
    "\n",
    "                    # compute UQI for each image\n",
    "                    for t in range(batch.shape[0]):\n",
    "                        uqi_loss_batch += uqi(reshaped_batch[t], reshaped_recon_data[t])\n",
    "\n",
    "                    # track average UQI of current batch\n",
    "                    uqi_loss_list.append(uqi_loss_batch / batch.shape[0])\n",
    "                    uqi_time_end = time.time()\n",
    "                    epoch_uqi_time += uqi_time_end - uqi_time_start\n",
    "                \n",
    "                # track MSE for current batch\n",
    "                loss_list.append(mse(batch, recon_data_prob))\n",
    "\n",
    "            # compute average MSE and std across batches\n",
    "            loss_avg = np.round(np.mean(loss_list), 4)\n",
    "            loss_std = np.round(np.std(loss_list), 4)\n",
    "            \n",
    "            if img_shape is not None:\n",
    "                # compute average UQI and std across batches\n",
    "                uqi_loss_avg = np.round(np.mean(uqi_loss_list), 4)\n",
    "                uqi_loss_std = np.round(np.std(uqi_loss_list), 4)\n",
    "                tot_uqi_time += epoch_uqi_time\n",
    "                \n",
    "            end = time.time()\n",
    "            eta = (end - start) * (max_epochs - i - 1) / (i + 1)\n",
    "\n",
    "            # print info about current epoch\n",
    "            if img_shape is not None:\n",
    "                print(f\"Epoch number {i+1} done. MSE: {loss_avg} \\xB1 {loss_std}. \"\n",
    "                    f\"UQI: {uqi_loss_avg} \\xB1 {uqi_loss_std} \"\n",
    "                    f\"Elapsed time: {np.round(end-start, 2)}s. ETA: {np.round(eta, 2)}s\")\n",
    "            else:\n",
    "                print(f\"Epoch number {i+1} done. MSE: {loss_avg} \\xB1 {loss_std}. \"\n",
    "                    f\"Elapsed time: {np.round(end-start, 2)}s. ETA: {np.round(eta, 2)}s\")\n",
    "        if img_shape is not None:\n",
    "            print(f\"Total time spent on UQI computation: {np.round(tot_uqi_time, 2)}s\")\n",
    "    \n",
    "    def reconstruct(self, ts_data: np.ndarray, seed:int = None) -> np.ndarray:\n",
    "        \"\"\"function to reconstruct the given input data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts_data : np.ndarray\n",
    "            data to reconstruct\n",
    "        seed : int, optional\n",
    "            seed to use during sampling. If None uses the RNG of the RBM, by default None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            the data reconstructed by the RBM\n",
    "        \"\"\"\n",
    "        # sample the input data\n",
    "        sampled_data = self.sample_data(ts_data, seed=seed)\n",
    "\n",
    "        # p(h0 | v0)\n",
    "        h_prob0 = sigmoid(np.dot(sampled_data, self._weights) + self._bh)\n",
    "        # sample hidden units\n",
    "        sampled_h0 = self.sample_data(h_prob0, seed=seed)\n",
    "        \n",
    "        # p(v1 | h0)\n",
    "        recon_data_prob = sigmoid(np.dot(sampled_h0, self._weights.T) + self._bv)\n",
    "        return recon_data_prob\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments and results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the RBM. The visible units are 784 (28*28), which means one unit for each pixel of an image. The hidden units are 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boltzmann_machine = RBM(v_units = tr_img_shape[1], h_units = 100, seed = 123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the RBM for 30 epochs, with a learning rate of 0.01, momentum of 0.9 and batchsize equal to 50.<br>\n",
    "These hyperparameters were picked by manually trying some combinations and picking the one that worked best, based on both quantitative measures and looking at some of the reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boltzmann_machine.reset()\n",
    "boltzmann_machine.fit(tr_images, \n",
    "                      max_epochs = 30, \n",
    "                      lr = 1e-2, \n",
    "                      momentum=0.9, \n",
    "                      batchsize = 50, \n",
    "                      img_shape = (28,28)\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct the test set on the trained RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_test = boltzmann_machine.reconstruct(ts_images)\n",
    "reconstructed_test_reshaped = np.reshape(reconstructed_test, (reconstructed_test.shape[0], 28, 28))\n",
    "ts_images_reshaped = np.reshape(ts_images, (ts_images.shape[0], 28, 28))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute MSE and average UQI between original test data and reconstucted data. Notice that, while the averages are close to the ones on the training set at the end of training, the standard deviations are higher. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE: 0.0172 ± 0.0679\n",
    "UQI: 0.7487 ± 0.1049"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_avg = np.round(mse(ts_images, reconstructed_test), 4)\n",
    "loss_std = np.round(((ts_images - reconstructed_test)**2).std(), 4)\n",
    "uqi_loss_list = []\n",
    "\n",
    "for i in range(ts_images_reshaped.shape[0]):\n",
    "    uqi_loss_list.append(uqi(ts_images_reshaped[i], reconstructed_test_reshaped[i]))\n",
    "uqi_loss_avg = np.round(np.mean(uqi_loss_list), 4)\n",
    "uqi_loss_std = np.round(np.std(uqi_loss_list), 4)\n",
    "\n",
    "print(f\"MSE: {loss_avg} \\xB1 {loss_std}\\n\"\n",
    "      f\"UQI: {uqi_loss_avg} \\xB1 {uqi_loss_std}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights of the hidden units of the RBM as images. Various parts of the digits can be seen in many weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = boltzmann_machine._weights.T\n",
    "weights = np.reshape(weights, (weights.shape[0], 28, 28))\n",
    "\n",
    "n_rows = 10\n",
    "n_columns = 10\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_columns, figsize=(10, 10))\n",
    "\n",
    "i = 0\n",
    "for row in range(n_rows):\n",
    "    for column in range(n_columns):\n",
    "        ax = axes[row, column]\n",
    "        ax.axis(\"off\")\n",
    "        ax.imshow(weights[i], cmap=\"gray\")\n",
    "        i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the average scores on the different digits we can see that the results are overall quite similar among them.<br>\n",
    "One outlier is the scores for the images for ones, as the UQI is surprisingly low while the MSE is lower than the other digits. This is possibly because of how the UQI is defined and it might simply not work well in this case, as comparing some of the images for ones to their reconstruction shows that they are more similar than one might expect when seeing the average UQI.<br>\n",
    "Sevens also have a similar behaviour to a lesser degree, while zeros have slightly higher UQI. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digit: 0. Average MSE: 0.0203. Average UQI: 0.849.\n",
    "Digit: 1. Average MSE: 0.0066. Average UQI: 0.5541.\n",
    "Digit: 2. Average MSE: 0.0223. Average UQI: 0.7806.\n",
    "Digit: 3. Average MSE: 0.0191. Average UQI: 0.7765.\n",
    "Digit: 4. Average MSE: 0.0167. Average UQI: 0.7559.\n",
    "Digit: 5. Average MSE: 0.0198. Average UQI: 0.7587.\n",
    "Digit: 6. Average MSE: 0.0189. Average UQI: 0.7733.\n",
    "Digit: 7. Average MSE: 0.0138. Average UQI: 0.7339.\n",
    "Digit: 8. Average MSE: 0.022. Average UQI: 0.7659.\n",
    "Digit: 9. Average MSE: 0.0147. Average UQI: 0.7695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    indices = np.where(test_labels==i)[0]\n",
    "    loss_avg = mse(ts_images[indices], reconstructed_test[indices])\n",
    "    uqi_avg = 0\n",
    "    for index in indices:\n",
    "        uqi_avg += uqi(ts_images_reshaped[index], reconstructed_test_reshaped[index])\n",
    "    uqi_avg /= indices.shape[0]\n",
    "    print(f\"Digit: {i}. Average MSE: {np.round(loss_avg, 4)}. Average UQI: {np.round(uqi_avg, 4)}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive examples of images reconstructed by the RBM for some handpicked test samples, one for each digit, with the MSE and UQI scores computed between the original image and the one reconstructed by the RBM.<br>\n",
    "The reconstructed images are overall quite close to the original ones, though they are all slightly more blurry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [10, 5, 1, 30, 27, 2798, 1831, 0, 110, 9]\n",
    "plot_images(indices, ts_images_reshaped, reconstructed_test_reshaped, test_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More handpicked examples for each digit, this time of samples that the RBM has trouble in reconstructing properly. While the general shape is generally still present there is either a lot of noise or missing parts. For the samples for the eight and the nine the reconstruction even looks more like a three, rather than the original digit.<br>\n",
    "In general, when the original digit is drawn in a way that deviates too much from the average, be it line thickness or shape, the reconstruction obtained from the RBM might suffer in quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [25, 40, 47, 87, 95, 167, 11, 64, 61, 479]\n",
    "plot_images(indices, ts_images_reshaped, reconstructed_test_reshaped, test_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some extra examples where the digits are written a bit differently than usual but the RBM is still able to reconstruct them rather accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [8, 4960, 4956, 6340, 4013, 2505, 7510]\n",
    "plot_images(indices, ts_images_reshaped, reconstructed_test_reshaped, test_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, in these examples the RBM is failing quite badly. In the first two the original images have dense areas, especially the 4, resulting in bad reconstructions. The middle three have the opposite problem, where the lines are too thin, making the RBM miss some parts while reconstructing the samples. Lastly, in the last two examples, the RBM outputs images that look quite believable but it changes the digit being represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [1551, 8107, 3749, 3751, 1941, 6511, 386]\n",
    "plot_images(indices, ts_images_reshaped, reconstructed_test_reshaped, test_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Remarks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment it was particularly interesting to play around with the image reconstruction. Considering that the model is relatively simple all things considered, the results are quite nice overall, as the reconstruction are similar to the original images for the most part. Additionally, with more in depth model selection RBMs might be able to get even better results, as I only tried a small number of combinations.<br>\n",
    "On picking a quantitative measure to get an objective idea of how good the results were, I decided to try out how the UQI behaves for this task as it looked rather promising while searching for image comparison specific measures. After looking at the various results obtained, the UQI turned out to be decent, but not quite perfect. The main example of this is the scores for the samples of digit 1, being extremely low despite the reconstructed images being very similar to the original ones. Moreover, it is unfortunately very slow to compute during training.<br>\n",
    "On the topic of speed, the training is somewhat slow and could stand to be optimized somehow, but the size of the net is almost certainly not helping, given that, even with a simple 28x28 image, 784 units are required just for the visible layer.<br>\n",
    "To get a better analysis one could also try more quantitative measures to better get an idea of how good the reconstructions are. Implementing more of the tips given in the [guide](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf) to training an RBM would probably lead to better results, too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISPR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0bb2bfd8162fb1fc2ac60c00546b252b3b1ae7a878ee25718b25f318c3edd43c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
